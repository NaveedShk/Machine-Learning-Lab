
Understanding of Assumptions in Machine Learning Classifiers
Support Vector Machine (SVM)
- Data should be scaled: SVMs assume that all features contribute equally, so feature scaling is important.
- Linear separability (for linear SVM): The model assumes that data can be separated with a hyperplane.
- Kernel choice: Assumes the chosen kernel function maps data into a space where classes become separable.
- Low noise: SVMs assume datasets are not extremely noisy since noise can affect margin placement.
K-Nearest Neighbors (KNN)
- Similarity assumption: Points that are close in feature space have similar output values.
- Distance metric relevance: Assumes the chosen distance metric (e.g., Euclidean) properly reflects similarity.
- Feature scaling: Assumes features are scaled so that none dominate the distance calculation.
- Locality assumption: Assumes local patterns exist in the data.
Decision Tree
- No linearity assumption: Decision trees assume the relationship between features and target may be non-linear.
- Feature independence within nodes: Splits assume one feature at a time can separate classes effectively.
- Sufficient depth: Assumes enough depth is available to capture patterns without overfitting.
- Data is fully observable: Assumes there are no hidden/unobserved variables impacting decisions.
Ensemble Learning (Bagging, Boosting, Random Forest)
- Weak learner assumption (Boosting): Assumes that weak learners slightly better than random can be combined to form a strong learner.
- Independence of models (Bagging): Assumes base learners are trained on different subsets to reduce variance.
- Decision tree assumptions: Ensembles using trees still inherit tree-related assumptions.
- Diversity assumption: Assumes that diverse models will improve overall accuracy.
